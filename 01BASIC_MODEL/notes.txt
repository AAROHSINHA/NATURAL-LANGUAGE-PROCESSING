1. Basic Multinomial Naive Bayes just sees the frequencies of words and probability if the token according to the frequency is of a specific class. For eg, if a word 'Assurance' is found more in class 1 than 0, and it is encountered in out test sentence, the model will give it some weight for class 1 only. Ofc its done in a more methodical and probability based model but this is how it works.
Now when we use this model with BOW, we are just providing frequencies without any relevent context. The sentence might be dual meaning like "The only bad thing about front page is is ends :)". First due to BOW we get the word "bad" which is most likely to appear in class 0 overall. Now Naive bayes, being naive is gonna map it to the class 0. And hence we get a wrong output.
The naive bayes is a good baseline model. Its not very simple and naive...works good enough for a baseline model but further optimization is necessary for the projects into better models and alternatives.